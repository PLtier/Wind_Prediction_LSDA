{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "# Data Pipelines & Data Analytics Life Cycle\n",
    "# Forecasting the Wind Power Production in Orkney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment and Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the environment with `venv`\n",
    "\n",
    "It is always recommended to create a new environment for any new project to avoid dependency issues and keep a clean working space. \n",
    "\n",
    "Step 1: Create a virtual environment in your project directory:\n",
    "\n",
    "python3 -m venv .venv\n",
    "\n",
    "\n",
    "Step 2: Activate the virtual environment:  \n",
    "For Linux/Mac:  \n",
    "source .venv/bin/activate  \n",
    "\n",
    "\n",
    "For Windows:  \n",
    ".venv\\Scripts\\activate  \n",
    "\n",
    "\n",
    "Step 3: Install the influxdb and MLFlow libraries using requirements.txt from https://github.itu.dk/Large-Scale-Data-Analysis-2025/python_env\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    "Once your environment is set up and dependencies are installed, you are ready to proceed with your project.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You absolutely need these\n",
    "from influxdb import InfluxDBClient\n",
    "import mlflow\n",
    "\n",
    "# You will probably need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# This are for example purposes. You may discard them if you don't use them.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from mlflow.models import infer_signature\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "### TODO -> HERE YOU CAN ADD ANY OTHER LIBRARIES YOU MAY NEED ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data with InfluxDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is stored in an [InfluxDB](https://www.influxdata.com/), which is a non-relational time-series database. InfluxDB can be queried using [InfluxQL](https://docs.influxdata.com/influxdb/v1.8/query_language/spec/), a \"SQL-like\" query language for time-series data. InfluxDB does not have tables with rows and columns, instead data is stored in measurements with fields and tags. <br><br>\n",
    "**NOTE:** <em>You don't need to know much about InfluxDB syntax, but if you are interested, feel free to browse around the [documentation](https://docs.influxdata.com/).</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for this assignment is stored in a database, with one table for the weather data and another for the power generation data. To do this, we first need to create an instance of the InfluxDB Client, that will allow us to query the needed data. Let's see how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from influxdb import InfluxDBClient\n",
    "\n",
    "# Set the needed parameters to connect to the database\n",
    "### THIS SHOULD NOT BE CHANGED ###\n",
    "settings = {\n",
    "    'host': 'influxus.itu.dk',\n",
    "    'port': 8086,\n",
    "    'username': 'lsda',\n",
    "    'password': 'icanonlyread'\n",
    "    }\n",
    "\n",
    "# Create an InfluxDB Client instance and select the orkney database\n",
    "### YOU DON'T NEED TO CHANGE ANYTHING HERE ###\n",
    "client = InfluxDBClient(host=settings['host'], port=settings['port'], username=settings['username'], password=settings['password'])\n",
    "client.switch_database('orkney')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained before, InfluxDB uses InfluxQL, a \"SQL-like\" syntax. We will use this to select the data we need from the correspondant tables, using our client instance. Then we can use an auxiliary function to convert the resulting set from the query into a Pandas Dataframe, making it easier to work with.<br><br>\n",
    "**NOTE:** <em>If you are curious to see how the resulting set from InfluxDB looks like, you can avoid using this function and printing the result.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to tranform the InfluxDB resulting set into a Dataframe\n",
    "### YOU DON'T NEED TO CHANGE ANYTHING HERE ###\n",
    "def set_to_dataframe(resulting_set):\n",
    "    \n",
    "    values = resulting_set.raw[\"series\"][0][\"values\"]\n",
    "    columns = resulting_set.raw[\"series\"][0][\"columns\"]\n",
    "    df = pd.DataFrame(values, columns=columns).set_index(\"time\")\n",
    "    df.index = pd.to_datetime(df.index) # Convert to datetime-index\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to retrieve the data.<br><br>\n",
    "Let's suppose we want the power generation and wind data from the last 90 days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Get the last 90 days of weather forecasts with the shortest lead time\u001b[39;00m\n\u001b[1;32m      9\u001b[0m wind_set  \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mquery(\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM MetForecasts where time > now()-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(days)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md and time <= now() and Lead_hours = \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     ) \u001b[38;5;66;03m# Query written in InfluxQL. We are retrieving all weather forecast data from 90 days back and with 1 lead hour.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m power_df \u001b[38;5;241m=\u001b[39m \u001b[43mset_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m wind_df \u001b[38;5;241m=\u001b[39m set_to_dataframe(wind_set)\n\u001b[1;32m     15\u001b[0m power_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpower_df.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m, in \u001b[0;36mset_to_dataframe\u001b[0;34m(resulting_set)\u001b[0m\n\u001b[1;32m      5\u001b[0m values \u001b[38;5;241m=\u001b[39m resulting_set\u001b[38;5;241m.\u001b[39mraw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m columns \u001b[38;5;241m=\u001b[39m resulting_set\u001b[38;5;241m.\u001b[39mraw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(values, columns\u001b[38;5;241m=\u001b[39mcolumns)\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;66;03m# Convert to datetime-index\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "days = 90 # -> You can change this to get any other range of days\n",
    "\n",
    "### YOU DON'T NEED TO CHANGE ANYTHING HERE ###\n",
    "power_set = client.query(\n",
    "    \"SELECT * FROM Generation where time > now()-\"+str(days)+\"d\"\n",
    "    ) # Query written in InfluxQL. We are retrieving all generation data from 90 days back.\n",
    "\n",
    "# Get the last 90 days of weather forecasts with the shortest lead time\n",
    "wind_set  = client.query(\n",
    "    \"SELECT * FROM MetForecasts where time > now()-\"+str(days)+\"d and time <= now() and Lead_hours = '1'\"\n",
    "    ) # Query written in InfluxQL. We are retrieving all weather forecast data from 90 days back and with 1 lead hour.\n",
    "\n",
    "power_df = set_to_dataframe(power_set)\n",
    "wind_df = set_to_dataframe(wind_set)\n",
    "power_df.to_csv('power_df.csv')\n",
    "wind_df.to_csv('wind_df.csv')\n",
    "power_df = pd.read_csv('power_df.csv', index_col='time')\n",
    "wind_df = pd.read_csv('wind_df.csv', index_col='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"2\">**NOTE:** <em>You don't need to change the query syntax, but it is useful if you try to make sense out of it.</em> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the resulting dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the hour timedeltas between subsequent rows\n",
    "power_df[:1000].index.to_series().transform(lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(power_df.iloc[:1000].index, power_df.iloc[:1000]['ANM'])\n",
    "plt.plot(power_df.iloc[:1000].index, power_df.iloc[:1000]['Total'])\n",
    "plt.plot(power_df.iloc[:1000].index, power_df.iloc[:1000]['Non-ANM'])\n",
    "# don't print x labels\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <em>This table contains three columns, but closer inspection will reveal a very straigh relationship between those three. Can you spot that?<br>\n",
    "We are clearly interested in the total power generation, regardless of the source type.</em>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(wind_df.iloc[:1000].index, wind_df.iloc[:1000]['Speed'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <em>This table contains four columns, but lead hours and source time are irrelevant here. Can you think why?</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <em>Look at the table's index. Do both data sources contain the same intervals? And if not, what problems could arise when merging the data?</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two dataframes, one with weather forecast and one with power generation. To do some analysis on the relationship between these two datasets, it might be useful to join (and align) the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining the data\n",
    "joined_dfs = power_df.join(wind_df, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining the two datasets with an inner join means keeping only those records that match their index. Although this will work, you may notice that most of our data is discarded due to the unmatching time intervals. You may want to explore other possible ways to merge the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO -> JOIN THE TWO DATASETS ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be also useful to plot our datasets to see what relationships they might hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplots\n",
    "fig, ax = plt.subplots(1,3, figsize=(25,4))\n",
    "\n",
    "# Speed and Power for the last 7 days\n",
    "ax[0].plot(joined_dfs[\"Speed\"].tail(int(7*24/3)), label=\"Speed\", color=\"blue\") # Since the datasets are joined every three hours, we need the last 7 days times 24 hours diveded by 3 hours\n",
    "ax[0].plot(joined_dfs[\"Total\"].tail(int(7*24/3)), label=\"Power\", color=\"tab:red\") # Since the datasets are joined every three hours, we need the last 7 days times 24 hours diveded by 3 hours\n",
    "ax[0].set_title(\"Windspeed & Power Generation over last 7 days\")\n",
    "ax[0].set_xlabel(\"Time\")\n",
    "ax[0].tick_params(axis='x', labelrotation = 45)\n",
    "ax[0].set_ylabel(\"Windspeed [m/s], Power [MW]\")\n",
    "ax[0].legend()\n",
    "\n",
    "# Speed vs Total (Power Curve nature)\n",
    "ax[1].scatter(joined_dfs[\"Speed\"], joined_dfs[\"Total\"])\n",
    "power_curve = joined_dfs.groupby(\"Speed\").median(numeric_only=True)[\"Total\"]\n",
    "ax[1].plot(power_curve.index, power_curve.values, \"k:\", label=\"Power Curve\")\n",
    "ax[1].legend()\n",
    "ax[1].set_title(\"Windspeed vs Power\")\n",
    "ax[1].set_ylabel(\"Power [MW]\")\n",
    "ax[1].set_xlabel(\"Windspeed [m/s]\")\n",
    "\n",
    "# Speed and Power per Wind Direction\n",
    "wind_grouped_by_direction = joined_dfs.groupby(\"Direction\").mean(numeric_only=True).reset_index()\n",
    "bar_width = 0.5\n",
    "x = np.arange(len(wind_grouped_by_direction.index))\n",
    "\n",
    "ax[2].bar(x, wind_grouped_by_direction.Total, width=0.5, label=\"Power\", color=\"tab:red\")\n",
    "ax[2].bar(x + bar_width, wind_grouped_by_direction.Speed, width=0.5, label=\"Speed\", color=\"blue\")\n",
    "ax[2].legend()\n",
    "ax[2].set_xticks(x)\n",
    "ax[2].set_xticklabels(wind_grouped_by_direction.Direction)\n",
    "ax[2].tick_params(axis='x', labelrotation = 45)\n",
    "ax[2].set_title(\"Speed and Power per Direction\");\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# create a difference plot\n",
    "plt.figure()\n",
    "\n",
    "y = wind_grouped_by_direction.Total / wind_grouped_by_direction.Speed\n",
    "\n",
    "plt.bar(wind_grouped_by_direction.Direction, y)\n",
    "# plot horizontal, a mean of y\n",
    "plt.axhline(y.mean(), color='r', linestyle='-')\n",
    "plt.show()\n",
    "\n",
    "# print standard deviation of y\n",
    "y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs.shape\n",
    "print(joined_dfs['Direction'].unique().size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional two plots:\n",
    "# Convert wind direction into categorical variable, and then plot the points given its three top principal components,\n",
    "# with color intensity given by the power generated.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One hot encode the wind direction\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_directions = encoder.fit_transform(joined_dfs[['Direction']])\n",
    "\n",
    "# Perform PCA to reduce dimensionality\n",
    "pca = PCA(n_components=3)\n",
    "principal_components = pca.fit_transform(encoded_directions)\n",
    "\n",
    "# Create a new DataFrame with the principal components and power generation\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2', 'PC3'])\n",
    "pca_df['Power'] = joined_dfs['Total'].values\n",
    "pca_df.drop_duplicates(subset=['PC1', 'PC2', 'PC3'],inplace=True)\n",
    "\n",
    "# Plot the points given its three top principal components, with color intensity given by the\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_directions = encoder.fit_transform(wind_df[['Direction']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print explained variability by each of the first three principal components\n",
    "\n",
    "# do 3d plot of the first three principal components\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure interactive plotting\n",
    "plt.ion()\n",
    "\n",
    "# Create rotatable 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# o = ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=labels, cmap=\"tab10\", alpha=0.5)\n",
    "o = ax.scatter(pca_df['PC1'], pca_df['PC2'], pca_df['PC3'], alpha=0.5)\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "\n",
    "plt.legend(*o.legend_elements(), title=\"Classes\")\n",
    "\n",
    "ax.view_init(10, 40)\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ACTUALLY, is there any sense to do PCA of categorical variables? I'm feeling that it's not the case. I'm not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't make any sense to do PCA by default. To verify, but that's what I'm going to write in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique rows, given the first three principal components\n",
    "pca_df.drop_duplicates(subset=['PC1', 'PC2', 'PC3']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each of components against each other using subplots\n",
    "fig, ax = plt.subplots(1, 3\n",
    "                       )\n",
    "\n",
    "for i, component in enumerate(['PC1', 'PC2', 'PC3']):\n",
    "    ax[i].scatter(pca_df[component], pca_df['Power'], alpha=0.5)\n",
    "    ax[i].set_xlabel(component)\n",
    "    ax[i].set_ylabel('Power')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print rows with empty values\n",
    "joined_dfs[joined_dfs.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_dfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not sure how to interpret this plot honestly. Saying that:\n",
    "# constant = wind_grouped_by_direction.Total / wind_grouped_by_direction.Speed helps anything, I'm not sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From that plot one could see that given an average windspeed, one could predict average power output, without considering the wind direction.\n",
    "# which suggests maybe it's not that important to consider wind direction in the model.\n",
    "# actually, a statistical test could be performed, because if these are means, these are normally distributed, and we could perform a t-test to see if the means are different.\n",
    "# so their ratio is distributed as cauchy, so can check that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another check in determining the importance of wind direction is checking the correlation between wind direction and speed.abs\n",
    "# Also, another one would be just plotting windspeed given different directions (using colouring) and see if there's any difference in pattern\n",
    "# perhaps would be the best to draw them one vs one, but there would be 25 combinations, so that's a lot of plots.\n",
    "\n",
    "# let's check the correlation between wind direction and speed.abs on wind_df\n",
    "\n",
    "one_hot_encoded = pd.get_dummies(wind_df['Direction'])\n",
    "\n",
    "# compute the correlation between wind speed and each of the one-hot encoded wind directions\n",
    "correlations = one_hot_encoded.apply(lambda x: wind_df['Speed'].corr(x))\n",
    "correlations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert wind directions to radians. Now it's categorical like E, W, N, S, ENE, ESE etc.\n",
    "# so we need to convert them to degrees to be able to plot them on a circle.\n",
    "# we can use the following mapping:\n",
    "converstion = {\n",
    "    'N': 0,\n",
    "    'NNE': 22.5,\n",
    "    'NE': 45,\n",
    "    'ENE': 67.5,\n",
    "    'E': 90,\n",
    "    'ESE': 112.5,\n",
    "    'SE': 135,\n",
    "    'SSE': 157.5,\n",
    "    'S': 180,\n",
    "    'SSW': 202.5,\n",
    "    'SW': 225,\n",
    "    'WSW': 247.5,\n",
    "    'W': 270,\n",
    "    'WNW': 292.5,\n",
    "    'NW': 315,\n",
    "    'NNW': 337.5\n",
    "}\n",
    "\n",
    "wind_df['Wx'] = wind_df['Direction'].transform(lambda x: np.cos(converstion[x] * np.pi / 180))\n",
    "wind_df['Wy'] = wind_df['Direction'].transform(lambda x: np.sin(converstion[x] * np.pi / 180))\n",
    "# if the values are close to 0, then make them 0\n",
    "wind_df['Wx'] = wind_df['Wx'].transform(lambda x: 0 if abs(x) < 1e-10 else x)\n",
    "wind_df['Wy'] = wind_df['Wy'].transform(lambda x: 0 if abs(x) < 1e-10 else x)\n",
    "# now, these together repesent the unit vector of the wind direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again: compute the correlation, and plot both Wx against Speed and Wy against Speed\n",
    "correlations = wind_df[['Wx', 'Wy', 'Speed']].corr()\n",
    "print(correlations)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0].scatter(wind_df['Wx'], wind_df['Speed'])\n",
    "ax[0].set_xlabel('Wx')\n",
    "ax[0].set_ylabel('Speed')\n",
    "\n",
    "ax[1].scatter(wind_df['Wy'], wind_df['Speed'])\n",
    "ax[1].set_xlabel('Wy')\n",
    "ax[1].set_ylabel('Speed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to encode these features?\n",
    "# As radians: imo very wrong, it's just 'sparse' ordinal encoding, for nominal data. Bad idea, though dimensionality is the smallest.\n",
    "# the second: one-hot encoding: I think could do fine, especially if some dimensionality reduction would be introduced. (PCA)\n",
    "# the last one: as a vector. That sound interesting, because here it should encapsulate all the necessary information in the smallest dimensionality possible.\n",
    "# Even though ordinality is introduced I guess it should be ounteracted by the ordinality of the second feature, so it should be fine.\n",
    "# But I'm not super sure. I will write an email to my professor asap. Gonna try with these two. Obviously here scaling shall not be applied.\n",
    "\n",
    "# Another issue: Should one scale categorical features? In my humble opinion, yes, because these still might have different variane - that's all.\n",
    "# But I need to think / talk about it haha. By default I will leave them in fact but I would do other way. But not sure. I think it's still pretty much worth it.\n",
    "# OK, let's think about it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web.archive.org/web/20220127105734/http://dr%C3%B8mst%C3%B8rre.dk/wp-content/wind/miller/windpower%20web/en/tour/wres/pwr.htm => super important!\n",
    "# Pitfalls in Using Power Curves\n",
    "# A power curve does not tell you how much power a wind turbine will produce at a certain average wind speed. You would not even be close, if you used that method!\n",
    "# Remember, that the energy content of the wind varies very strongly with the wind speed, as we saw in the section on the energy in the wind. So, it matters a lot how that average came about, i.e. if winds vary a lot, or if the wind blows at a relatively constant speed.\n",
    "# Also, you may remember from the example in the section on the power density function , that most of the wind energy is available at wind speeds which are twice the most common wind speed at the site.\n",
    "# Finally, we need to account for the fact that the turbine may not be running at standard air pressure and temperature, and consequently make corrections for changes in the density of air."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <em>These plots should already give us an intuition of the different relationships between features. It seems clear that there is a positive relationship between the wind speed and the power generation from the turbines, as we obviously suspected. But that relationship is not completely linear. Can you spot that? Finally, it seems like the power generation also depends of where the winds is coming from. Maybe this could also be a useful feature.</em> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much data back: let's try 30, 90 days and 1 year back perhaps? Let's try with 90 days first and a year and then talk. For that I would just perform experiments\n",
    "# haha :)\n",
    "# More days means more data, but obviously in the past some other could be in place - I mean, climate change, global warming etc. Methaphorically speaking, data can 'expire'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to plot the relationship between wind speed and power generation we have performed a very simple join with the two datasets. But since the intervals are not the same, a lot of data is discarded (<em>can you spot where in the code this happens?</em>). You may want to explore other ways to merge the data sources to minimize the loss of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO -> DO SOME EXTRA EDA ON THE DATA ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model:\n",
    "1. Pull data (7, 90, a year). Log time. You can also log dataset haha, to have it as a point of reference. Log the dataset on its own :)\n",
    "2. Drop missing and drop outliers. (I drop missing: enough data, drop outliers - standard procedure.) Check for missing\n",
    "3. Do the cross val. For 7 days, use 4 weeks. For 90 years, use the whole year. For one year, use another year.\n",
    "4. Encode features and scale them appropriately. Log the way it's been done haha, including info on them.\n",
    "5. Output the score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wind_power.dataset import InfluxDBClientWrapper, get_power_and_wind_data\n",
    "with InfluxDBClientWrapper() as client:\n",
    "    power_df, wind_df, today = get_power_and_wind_data(client, days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Direction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Lead_hours",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Source_time",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Speed",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "efb9b0ac-3cb6-4278-b73e-4588b22524ef",
       "rows": [
        [
         "2024-11-28 12:00:00+00:00",
         "S",
         "1",
         "1732788000",
         "4.91744"
        ],
        [
         "2024-11-28 15:00:00+00:00",
         "SSE",
         "1",
         "1732798800",
         "8.04672"
        ],
        [
         "2024-11-28 18:00:00+00:00",
         "S",
         "1",
         "1732809600",
         "12.07008"
        ],
        [
         "2024-11-28 21:00:00+00:00",
         "S",
         "1",
         "1732820400",
         "15.19936"
        ],
        [
         "2024-11-29 00:00:00+00:00",
         "S",
         "1",
         "1732831200",
         "13.85824"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Lead_hours</th>\n",
       "      <th>Source_time</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-28 12:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732788000</td>\n",
       "      <td>4.91744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 15:00:00+00:00</th>\n",
       "      <td>SSE</td>\n",
       "      <td>1</td>\n",
       "      <td>1732798800</td>\n",
       "      <td>8.04672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 18:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732809600</td>\n",
       "      <td>12.07008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 21:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732820400</td>\n",
       "      <td>15.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 00:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732831200</td>\n",
       "      <td>13.85824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Direction  Lead_hours  Source_time     Speed\n",
       "time                                                                  \n",
       "2024-11-28 12:00:00+00:00         S           1   1732788000   4.91744\n",
       "2024-11-28 15:00:00+00:00       SSE           1   1732798800   8.04672\n",
       "2024-11-28 18:00:00+00:00         S           1   1732809600  12.07008\n",
       "2024-11-28 21:00:00+00:00         S           1   1732820400  15.19936\n",
       "2024-11-29 00:00:00+00:00         S           1   1732831200  13.85824"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power_df = pd.read_csv('./power_df.csv', index_col='time', usecols=['Total', 'time'])\n",
    "wind_df = pd.read_csv('./wind_df.csv', index_col='time')\n",
    "power_df.head()\n",
    "wind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wind_power.features import wind_direction_symbol_to_vector_ct, wind_direction_symbol_to_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_wind_df = wind_direction_symbol_to_vector_ct.fit_transform(wind_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of wind_power.features failed: Traceback (most recent call last):\n",
      "  File \"/Users/mpj-projects/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/mpj-projects/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/Users/mpj-projects/.pyenv/versions/3.11.7/lib/python3.11/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 621, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/Users/mpj-projects/repositories/a1_lsda2025/wind_power/features.py\", line 72, in <module>\n",
      "    wind_direction_symbol_to_onehot.set_output(transform=\"pandas\")\n",
      "  File \"/Users/mpj-projects/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 373, in set_output\n",
      "    for trans in transformers:\n",
      "  File \"/Users/mpj-projects/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py\", line 368, in <genexpr>\n",
      "    for _, trans, _ in chain(\n",
      "        ^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 3)\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'FunctionTransformer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwind_direction_symbol_to_onehot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwind_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:965\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# set n_features_in_ attribute\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_transformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:501\u001b[0m, in \u001b[0;36mColumnTransformer._validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m names, transformers, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# validate names\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_names(names)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FunctionTransformer' object is not iterable"
     ]
    }
   ],
   "source": [
    "wind_direction_symbol_to_onehot.fit_transform(wind_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "pipeline = make_pipeline(wind_direction_symbol_to_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FunctionTransformer' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from sklearn.preprocessing import OneHotEncoder\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# encoder = OneHotEncoder(sparse_output=False)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwind_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# encoder.fit_transform(wind_df)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:922\u001b[0m, in \u001b[0;36mColumnTransformer.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    919\u001b[0m _raise_for_params(params, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# we use fit_transform to make sure to set sparse_output_ (for which we\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# need the transformed data) to have consistent output type in predict\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:965\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;66;03m# set n_features_in_ attribute\u001b[39;00m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_n_features(X, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 965\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_transformers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_column_callables(X)\n",
      "File \u001b[0;32m~/.pyenv/versions/a1_lsda2025/lib/python3.11/site-packages/sklearn/compose/_column_transformer.py:501\u001b[0m, in \u001b[0;36mColumnTransformer._validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformers:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m names, transformers, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;66;03m# validate names\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_names(names)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'FunctionTransformer' object is not iterable"
     ]
    }
   ],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# encoder = OneHotEncoder(sparse_output=False)\n",
    "pipeline.fit(wind_df)\n",
    "# encoder.fit_transform(wind_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Direction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Wx",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Wy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Lead_hours",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Source_time",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Speed",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "2826f03c-27c7-4a6e-bf4f-614699ae4ae7",
       "rows": [
        [
         "2024-11-28 12:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732788000",
         "4.91744"
        ],
        [
         "2024-11-28 15:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1732798800",
         "8.04672"
        ],
        [
         "2024-11-28 18:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732809600",
         "12.07008"
        ],
        [
         "2024-11-28 21:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732820400",
         "15.19936"
        ],
        [
         "2024-11-29 00:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732831200",
         "13.85824"
        ],
        [
         "2024-11-29 03:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1732842000",
         "11.176"
        ],
        [
         "2024-11-29 06:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732852800",
         "8.9408"
        ],
        [
         "2024-11-29 09:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732863600",
         "8.9408"
        ],
        [
         "2024-11-29 12:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732874400",
         "5.81152"
        ],
        [
         "2024-11-29 15:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732885200",
         "9.83488"
        ],
        [
         "2024-11-29 18:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732896000",
         "8.9408"
        ],
        [
         "2024-11-29 21:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732906800",
         "11.176"
        ],
        [
         "2024-11-30 00:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732917600",
         "8.04672"
        ],
        [
         "2024-11-30 03:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1732928400",
         "7.15264"
        ],
        [
         "2024-11-30 06:00:00+00:00",
         "SW",
         "-0.7071067811865477",
         "-0.7071067811865474",
         "1",
         "1732939200",
         "8.04672"
        ],
        [
         "2024-11-30 09:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1732950000",
         "9.83488"
        ],
        [
         "2024-11-30 12:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1732960800",
         "4.91744"
        ],
        [
         "2024-11-30 15:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732971600",
         "7.15264"
        ],
        [
         "2024-11-30 18:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732982400",
         "8.9408"
        ],
        [
         "2024-11-30 21:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1732993200",
         "12.07008"
        ],
        [
         "2024-12-01 00:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733004000",
         "12.96416"
        ],
        [
         "2024-12-01 03:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733014800",
         "12.96416"
        ],
        [
         "2024-12-01 06:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1733025600",
         "11.176"
        ],
        [
         "2024-12-01 09:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733036400",
         "8.9408"
        ],
        [
         "2024-12-01 12:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1733047200",
         "8.9408"
        ],
        [
         "2024-12-01 15:00:00+00:00",
         "SW",
         "-0.7071067811865477",
         "-0.7071067811865474",
         "1",
         "1733058000",
         "7.15264"
        ],
        [
         "2024-12-01 18:00:00+00:00",
         "NNW",
         "0.9238795325112868",
         "-0.38268343236508956",
         "1",
         "1733068800",
         "8.9408"
        ],
        [
         "2024-12-01 21:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733079600",
         "13.85824"
        ],
        [
         "2024-12-02 00:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733090400",
         "13.85824"
        ],
        [
         "2024-12-02 03:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733101200",
         "15.19936"
        ],
        [
         "2024-12-02 06:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733112000",
         "16.98752"
        ],
        [
         "2024-12-02 09:00:00+00:00",
         "NNE",
         "0.9238795325112867",
         "0.3826834323650898",
         "1",
         "1733122800",
         "12.07008"
        ],
        [
         "2024-12-02 12:00:00+00:00",
         "NNE",
         "0.9238795325112867",
         "0.3826834323650898",
         "1",
         "1733133600",
         "16.09344"
        ],
        [
         "2024-12-02 15:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733144400",
         "11.176"
        ],
        [
         "2024-12-02 18:00:00+00:00",
         "N",
         "1.0",
         "0.0",
         "1",
         "1733155200",
         "9.83488"
        ],
        [
         "2024-12-02 21:00:00+00:00",
         "NNE",
         "0.9238795325112867",
         "0.3826834323650898",
         "1",
         "1733166000",
         "8.04672"
        ],
        [
         "2024-12-03 00:00:00+00:00",
         "NNE",
         "0.9238795325112867",
         "0.3826834323650898",
         "1",
         "1733176800",
         "5.81152"
        ],
        [
         "2024-12-03 03:00:00+00:00",
         "NE",
         "0.7071067811865476",
         "0.7071067811865475",
         "1",
         "1733187600",
         "3.12928"
        ],
        [
         "2024-12-03 06:00:00+00:00",
         "E",
         "0.0",
         "1.0",
         "1",
         "1733198400",
         "1.78816"
        ],
        [
         "2024-12-03 09:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1733209200",
         "4.91744"
        ],
        [
         "2024-12-03 12:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733220000",
         "8.04672"
        ],
        [
         "2024-12-03 15:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733230800",
         "12.07008"
        ],
        [
         "2024-12-03 18:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733241600",
         "11.176"
        ],
        [
         "2024-12-03 21:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1733252400",
         "9.83488"
        ],
        [
         "2024-12-04 00:00:00+00:00",
         "WNW",
         "0.38268343236509",
         "-0.9238795325112866",
         "1",
         "1733263200",
         "11.176"
        ],
        [
         "2024-12-04 03:00:00+00:00",
         "WSW",
         "-0.3826834323650895",
         "-0.9238795325112868",
         "1",
         "1733274000",
         "11.176"
        ],
        [
         "2024-12-04 06:00:00+00:00",
         "W",
         "0.0",
         "-1.0",
         "1",
         "1733284800",
         "4.91744"
        ],
        [
         "2024-12-04 09:00:00+00:00",
         "WSW",
         "-0.3826834323650895",
         "-0.9238795325112868",
         "1",
         "1733295600",
         "7.15264"
        ],
        [
         "2024-12-04 12:00:00+00:00",
         "SSW",
         "-0.923879532511287",
         "-0.3826834323650892",
         "1",
         "1733306400",
         "7.15264"
        ],
        [
         "2024-12-04 15:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1733317200",
         "8.9408"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 720
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Wx</th>\n",
       "      <th>Wy</th>\n",
       "      <th>Lead_hours</th>\n",
       "      <th>Source_time</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-28 12:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732788000</td>\n",
       "      <td>4.91744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 15:00:00+00:00</th>\n",
       "      <td>SSE</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>1</td>\n",
       "      <td>1732798800</td>\n",
       "      <td>8.04672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 18:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732809600</td>\n",
       "      <td>12.07008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 21:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732820400</td>\n",
       "      <td>15.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 00:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732831200</td>\n",
       "      <td>13.85824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-25 21:00:00+00:00</th>\n",
       "      <td>WNW</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>1</td>\n",
       "      <td>1740510000</td>\n",
       "      <td>9.83488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 00:00:00+00:00</th>\n",
       "      <td>NW</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>1</td>\n",
       "      <td>1740520800</td>\n",
       "      <td>5.81152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 03:00:00+00:00</th>\n",
       "      <td>W</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1740531600</td>\n",
       "      <td>5.81152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 06:00:00+00:00</th>\n",
       "      <td>W</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1740542400</td>\n",
       "      <td>5.81152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 09:00:00+00:00</th>\n",
       "      <td>SSW</td>\n",
       "      <td>-0.923880</td>\n",
       "      <td>-0.382683</td>\n",
       "      <td>1</td>\n",
       "      <td>1740553200</td>\n",
       "      <td>4.02336</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Direction        Wx        Wy  Lead_hours  \\\n",
       "time                                                                  \n",
       "2024-11-28 12:00:00+00:00         S -1.000000  0.000000           1   \n",
       "2024-11-28 15:00:00+00:00       SSE -0.923880  0.382683           1   \n",
       "2024-11-28 18:00:00+00:00         S -1.000000  0.000000           1   \n",
       "2024-11-28 21:00:00+00:00         S -1.000000  0.000000           1   \n",
       "2024-11-29 00:00:00+00:00         S -1.000000  0.000000           1   \n",
       "...                             ...       ...       ...         ...   \n",
       "2025-02-25 21:00:00+00:00       WNW  0.382683 -0.923880           1   \n",
       "2025-02-26 00:00:00+00:00        NW  0.707107 -0.707107           1   \n",
       "2025-02-26 03:00:00+00:00         W  0.000000 -1.000000           1   \n",
       "2025-02-26 06:00:00+00:00         W  0.000000 -1.000000           1   \n",
       "2025-02-26 09:00:00+00:00       SSW -0.923880 -0.382683           1   \n",
       "\n",
       "                           Source_time     Speed  \n",
       "time                                              \n",
       "2024-11-28 12:00:00+00:00   1732788000   4.91744  \n",
       "2024-11-28 15:00:00+00:00   1732798800   8.04672  \n",
       "2024-11-28 18:00:00+00:00   1732809600  12.07008  \n",
       "2024-11-28 21:00:00+00:00   1732820400  15.19936  \n",
       "2024-11-29 00:00:00+00:00   1732831200  13.85824  \n",
       "...                                ...       ...  \n",
       "2025-02-25 21:00:00+00:00   1740510000   9.83488  \n",
       "2025-02-26 00:00:00+00:00   1740520800   5.81152  \n",
       "2025-02-26 03:00:00+00:00   1740531600   5.81152  \n",
       "2025-02-26 06:00:00+00:00   1740542400   5.81152  \n",
       "2025-02-26 09:00:00+00:00   1740553200   4.02336  \n",
       "\n",
       "[720 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_wind_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "Direction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Wx",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Wy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Lead_hours",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source_time",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Speed",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "4b39e470-cb1d-4c65-8c14-5b55d0b83f55",
       "rows": [
        [
         "2024-11-28 18:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732809600",
         "12.070079999999999"
        ],
        [
         "2024-11-28 21:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732820400",
         "15.19936"
        ],
        [
         "2024-11-29 00:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732831200",
         "13.85824"
        ],
        [
         "2024-11-29 03:00:00+00:00",
         "SSE",
         "-0.9238795325112867",
         "0.38268343236508984",
         "1",
         "1732842000",
         "11.176"
        ],
        [
         "2024-11-29 06:00:00+00:00",
         "S",
         "-1.0",
         "0.0",
         "1",
         "1732852800",
         "8.9408"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Wx</th>\n",
       "      <th>Wy</th>\n",
       "      <th>Lead_hours</th>\n",
       "      <th>Source_time</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-28 18:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732809600</td>\n",
       "      <td>12.07008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 21:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732820400</td>\n",
       "      <td>15.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 00:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732831200</td>\n",
       "      <td>13.85824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 03:00:00+00:00</th>\n",
       "      <td>SSE</td>\n",
       "      <td>-0.92388</td>\n",
       "      <td>0.382683</td>\n",
       "      <td>1</td>\n",
       "      <td>1732842000</td>\n",
       "      <td>11.17600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 06:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1732852800</td>\n",
       "      <td>8.94080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Direction       Wx        Wy Lead_hours  \\\n",
       "time                                                                \n",
       "2024-11-28 18:00:00+00:00         S -1.00000  0.000000          1   \n",
       "2024-11-28 21:00:00+00:00         S -1.00000  0.000000          1   \n",
       "2024-11-29 00:00:00+00:00         S -1.00000  0.000000          1   \n",
       "2024-11-29 03:00:00+00:00       SSE -0.92388  0.382683          1   \n",
       "2024-11-29 06:00:00+00:00         S -1.00000  0.000000          1   \n",
       "\n",
       "                           Source_time     Speed  \n",
       "time                                              \n",
       "2024-11-28 18:00:00+00:00   1732809600  12.07008  \n",
       "2024-11-28 21:00:00+00:00   1732820400  15.19936  \n",
       "2024-11-29 00:00:00+00:00   1732831200  13.85824  \n",
       "2024-11-29 03:00:00+00:00   1732842000  11.17600  \n",
       "2024-11-29 06:00:00+00:00   1732852800   8.94080  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_wind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ANM', 'Non-ANM', 'Total'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "time",
         "rawType": "datetime64[ns, UTC]",
         "type": "unknown"
        },
        {
         "name": "Direction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Lead_hours",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Source_time",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Speed",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "b7159ec6-5a3a-4dd7-8820-cb5930617044",
       "rows": [
        [
         "2024-11-28 18:00:00+00:00",
         "S",
         "1",
         "1732809600",
         "12.070079999999999"
        ],
        [
         "2024-11-28 21:00:00+00:00",
         "S",
         "1",
         "1732820400",
         "15.19936"
        ],
        [
         "2024-11-29 00:00:00+00:00",
         "S",
         "1",
         "1732831200",
         "13.85824"
        ],
        [
         "2024-11-29 03:00:00+00:00",
         "SSE",
         "1",
         "1732842000",
         "11.176"
        ],
        [
         "2024-11-29 06:00:00+00:00",
         "S",
         "1",
         "1732852800",
         "8.9408"
        ],
        [
         "2024-11-29 09:00:00+00:00",
         "S",
         "1",
         "1732863600",
         "8.9408"
        ],
        [
         "2024-11-29 12:00:00+00:00",
         "S",
         "1",
         "1732874400",
         "5.81152"
        ],
        [
         "2024-11-29 15:00:00+00:00",
         "S",
         "1",
         "1732885200",
         "9.83488"
        ],
        [
         "2024-11-29 18:00:00+00:00",
         "S",
         "1",
         "1732896000",
         "8.9408"
        ],
        [
         "2024-11-29 21:00:00+00:00",
         "S",
         "1",
         "1732906800",
         "11.176"
        ],
        [
         "2024-11-30 00:00:00+00:00",
         "S",
         "1",
         "1732917600",
         "8.04672"
        ],
        [
         "2024-11-30 03:00:00+00:00",
         "SSW",
         "1",
         "1732928400",
         "7.15264"
        ],
        [
         "2024-11-30 06:00:00+00:00",
         "SW",
         "1",
         "1732939200",
         "8.04672"
        ],
        [
         "2024-11-30 09:00:00+00:00",
         "SSW",
         "1",
         "1732950000",
         "9.83488"
        ],
        [
         "2024-11-30 12:00:00+00:00",
         "SSW",
         "1",
         "1732960800",
         "4.91744"
        ],
        [
         "2024-11-30 15:00:00+00:00",
         "S",
         "1",
         "1732971600",
         "7.15264"
        ],
        [
         "2024-11-30 18:00:00+00:00",
         "S",
         "1",
         "1732982400",
         "8.9408"
        ],
        [
         "2024-11-30 21:00:00+00:00",
         "SSE",
         "1",
         "1732993200",
         "12.070079999999999"
        ],
        [
         "2024-12-01 00:00:00+00:00",
         "S",
         "1",
         "1733004000",
         "12.96416"
        ],
        [
         "2024-12-01 03:00:00+00:00",
         "S",
         "1",
         "1733014800",
         "12.96416"
        ],
        [
         "2024-12-01 06:00:00+00:00",
         "SSW",
         "1",
         "1733025600",
         "11.176"
        ],
        [
         "2024-12-01 09:00:00+00:00",
         "S",
         "1",
         "1733036400",
         "8.9408"
        ],
        [
         "2024-12-01 12:00:00+00:00",
         "SSW",
         "1",
         "1733047200",
         "8.9408"
        ],
        [
         "2024-12-01 15:00:00+00:00",
         "SW",
         "1",
         "1733058000",
         "7.15264"
        ],
        [
         "2024-12-01 18:00:00+00:00",
         "NNW",
         "1",
         "1733068800",
         "8.9408"
        ],
        [
         "2024-12-01 21:00:00+00:00",
         "N",
         "1",
         "1733079600",
         "13.85824"
        ],
        [
         "2024-12-02 00:00:00+00:00",
         "N",
         "1",
         "1733090400",
         "13.85824"
        ],
        [
         "2024-12-02 03:00:00+00:00",
         "N",
         "1",
         "1733101200",
         "15.19936"
        ],
        [
         "2024-12-02 06:00:00+00:00",
         "N",
         "1",
         "1733112000",
         "16.98752"
        ],
        [
         "2024-12-02 09:00:00+00:00",
         "NNE",
         "1",
         "1733122800",
         "12.070079999999999"
        ],
        [
         "2024-12-02 12:00:00+00:00",
         "NNE",
         "1",
         "1733133600",
         "16.09344"
        ],
        [
         "2024-12-02 15:00:00+00:00",
         "N",
         "1",
         "1733144400",
         "11.176"
        ],
        [
         "2024-12-02 18:00:00+00:00",
         "N",
         "1",
         "1733155200",
         "9.83488"
        ],
        [
         "2024-12-02 21:00:00+00:00",
         "NNE",
         "1",
         "1733166000",
         "8.04672"
        ],
        [
         "2024-12-03 00:00:00+00:00",
         "NNE",
         "1",
         "1733176800",
         "5.81152"
        ],
        [
         "2024-12-03 03:00:00+00:00",
         "NE",
         "1",
         "1733187600",
         "3.12928"
        ],
        [
         "2024-12-03 06:00:00+00:00",
         "E",
         "1",
         "1733198400",
         "1.78816"
        ],
        [
         "2024-12-03 09:00:00+00:00",
         "SSE",
         "1",
         "1733209200",
         "4.91744"
        ],
        [
         "2024-12-03 12:00:00+00:00",
         "S",
         "1",
         "1733220000",
         "8.04672"
        ],
        [
         "2024-12-03 15:00:00+00:00",
         "S",
         "1",
         "1733230800",
         "12.070079999999999"
        ],
        [
         "2024-12-03 18:00:00+00:00",
         "S",
         "1",
         "1733241600",
         "11.176"
        ],
        [
         "2024-12-03 21:00:00+00:00",
         "S",
         "1",
         "1733252400",
         "9.83488"
        ],
        [
         "2024-12-04 00:00:00+00:00",
         "WNW",
         "1",
         "1733263200",
         "11.176"
        ],
        [
         "2024-12-04 03:00:00+00:00",
         "WSW",
         "1",
         "1733274000",
         "11.176"
        ],
        [
         "2024-12-04 06:00:00+00:00",
         "W",
         "1",
         "1733284800",
         "4.91744"
        ],
        [
         "2024-12-04 09:00:00+00:00",
         "WSW",
         "1",
         "1733295600",
         "7.15264"
        ],
        [
         "2024-12-04 12:00:00+00:00",
         "SSW",
         "1",
         "1733306400",
         "7.15264"
        ],
        [
         "2024-12-04 15:00:00+00:00",
         "SSE",
         "1",
         "1733317200",
         "8.9408"
        ],
        [
         "2024-12-04 18:00:00+00:00",
         "SSE",
         "1",
         "1733328000",
         "13.85824"
        ],
        [
         "2024-12-04 21:00:00+00:00",
         "SSE",
         "1",
         "1733338800",
         "21.01088"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 720
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Direction</th>\n",
       "      <th>Lead_hours</th>\n",
       "      <th>Source_time</th>\n",
       "      <th>Speed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-28 18:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732809600</td>\n",
       "      <td>12.07008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-28 21:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732820400</td>\n",
       "      <td>15.19936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 00:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732831200</td>\n",
       "      <td>13.85824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 03:00:00+00:00</th>\n",
       "      <td>SSE</td>\n",
       "      <td>1</td>\n",
       "      <td>1732842000</td>\n",
       "      <td>11.17600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-29 06:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1732852800</td>\n",
       "      <td>8.94080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 03:00:00+00:00</th>\n",
       "      <td>W</td>\n",
       "      <td>1</td>\n",
       "      <td>1740531600</td>\n",
       "      <td>5.81152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 06:00:00+00:00</th>\n",
       "      <td>W</td>\n",
       "      <td>1</td>\n",
       "      <td>1740542400</td>\n",
       "      <td>5.81152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 09:00:00+00:00</th>\n",
       "      <td>SSW</td>\n",
       "      <td>1</td>\n",
       "      <td>1740553200</td>\n",
       "      <td>4.02336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 12:00:00+00:00</th>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1740564000</td>\n",
       "      <td>4.02336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-02-26 15:00:00+00:00</th>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1740574800</td>\n",
       "      <td>3.12928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Direction Lead_hours  Source_time     Speed\n",
       "time                                                                 \n",
       "2024-11-28 18:00:00+00:00         S          1   1732809600  12.07008\n",
       "2024-11-28 21:00:00+00:00         S          1   1732820400  15.19936\n",
       "2024-11-29 00:00:00+00:00         S          1   1732831200  13.85824\n",
       "2024-11-29 03:00:00+00:00       SSE          1   1732842000  11.17600\n",
       "2024-11-29 06:00:00+00:00         S          1   1732852800   8.94080\n",
       "...                             ...        ...          ...       ...\n",
       "2025-02-26 03:00:00+00:00         W          1   1740531600   5.81152\n",
       "2025-02-26 06:00:00+00:00         W          1   1740542400   5.81152\n",
       "2025-02-26 09:00:00+00:00       SSW          1   1740553200   4.02336\n",
       "2025-02-26 12:00:00+00:00         S          1   1740564000   4.02336\n",
       "2025-02-26 15:00:00+00:00         E          1   1740574800   3.12928\n",
       "\n",
       "[720 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_df\n",
    "# Just to mention outliers in the report that I didn't handle them because it seems\n",
    "# too complicated for the scope of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wind_power.config import DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'x' from 'wind_power.features' (/Users/mpj-projects/repositories/a1_lsda2025/wind_power/features.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwind_power\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m x\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'x' from 'wind_power.features' (/Users/mpj-projects/repositories/a1_lsda2025/wind_power/features.py)"
     ]
    }
   ],
   "source": [
    "from wind_power.features import x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'wind_direction_symbol_to_onehot' from 'wind_power.features' (/Users/mpj-projects/repositories/a1_lsda2025/wind_power/features.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwind_power\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wind_direction_symbol_to_onehot\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# wind_direction_symbol_to_onehot.fit_transform(wind_df)\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'wind_direction_symbol_to_onehot' from 'wind_power.features' (/Users/mpj-projects/repositories/a1_lsda2025/wind_power/features.py)"
     ]
    }
   ],
   "source": [
    "from wind_power.features import wind_direction_symbol_to_onehot\n",
    "\n",
    "# wind_direction_symbol_to_onehot.fit_transform(wind_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pipeline and data transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, we need to construct the pipeline to process this data and pass it to our Machine Learning model. For this, you may find useful the Pipeline class from Scikit-Learn.\n",
    "\n",
    "This class applies a list of transforms to your data, and pass the final state to an estimator (your model). Intermediate steps of the pipeline must be transforms, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. \n",
    "\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a '__'. \n",
    "\n",
    "You can find more information about Scikit-Learn's Pipeline [here](https://scikit-learn.org/stable/modules/compose.html#pipeline)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A very basic pipeline\n",
    "pipeline_example = Pipeline([\n",
    "    # Transformations\n",
    "    (\"Scaler\", StandardScaler()),\n",
    "    # Estimator\n",
    "    (\"Linear Regression\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO -> CREATE YOUR OWN PIPELINE ###\n",
    "# Create your pipeline with the desired transformers\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    # Transformer 1\n",
    "    # Transformer 2\n",
    "    # ...\n",
    "    # Final estimator\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a preprocessing pipeline ready, along with the final estimator, you may want to know how well your model performs. Choose the method you prefer, with special attention to the selected metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our feature variables and our target variable.\n",
    "joined_dfs = power_df.join(wind_df).dropna()\n",
    "\n",
    "X = joined_dfs[\"Speed\"].values.reshape(-1,1)\n",
    "y = joined_dfs[\"Total\"].values.reshape(-1,1)\n",
    "\n",
    "# Split the data so we can test how well our model performs in unseen data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) # -> You might want to use another split method\n",
    "\n",
    "# Train our model\n",
    "pipeline_example.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model, using MAE as a metric\n",
    "mae = mean_absolute_error(pipeline_example.predict(X_test), y_test)\n",
    "print(mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HINTS:** <em>Pay special attention to this type of data: We are dealing with Time series data (i.e. data that is recorded over consistent intervals of time). It might be a good idea not to **randomly** split the data, since it wouldn't respect the temporal order and may cause data-leakage, unintentionally inferring the trend of future samples.</em> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your preferred method to evaluate your model\n",
    "### TODO -> SPLIT THE DATA INTO TRAIN AND TEST SETS, AND EVALUATE YOUR MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tracking your experiments with MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a working model with a certain accuracy. But wouldn't it be better to try different parameters and different models before deciding for one? <br><br>\n",
    "This is exactly what we will do using the MLFlow library. MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. This will allow us for easy comparison of all our model experiments. <br>\n",
    "\n",
    "**NOTE:**<em> Don't forget to check the [MLFlow documentation](https://mlflow.org/docs/latest/index.html) to learn more about the library.</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using MLFlow locally to log our experiments, we need to start a \"local server\". We can do this easily by running the following in our command line interface:\n",
    "\n",
    "```\n",
    "mlflow server\n",
    "```\n",
    "\n",
    "For example, using PowerShell, it should look like this:\n",
    "\n",
    "![MLFlow Server](https://github.com/ginofazzi/testing/raw/0d3ffce58ee12f6d1c86b6a3c2ac628314e7b82e/mlflow_server.png)\n",
    "\n",
    "In this example, we can see that the server is located at our localhost 127.0.0.1, port 5000. We will use this information to indicate MLFlow where to save our experiment details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an MLflow run\n",
    "mlflow.sklearn.autolog() # This is to help us track scikit learn metrics.\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\") # We set the MLFlow UI to display in our local host.\n",
    "\n",
    "# Set the experiment and run name\n",
    "experiment_name = \"LinearRegression-Example\" # I suggest using a different experiment for each model\n",
    "run_name = \"Simple_regression\" # I suggest using a different run name for each tried parameter\n",
    "\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "with mlflow.start_run(run_name=run_name) as run:\n",
    "    \n",
    "    # Train our model\n",
    "    pipeline_example.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the model, using MAE as a metric\n",
    "    predictions = pipeline_example.predict(X_test)\n",
    "    mae = mean_absolute_error(predictions, y_test)\n",
    "\n",
    "    mlflow.log_metric(\"MAE\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO -> SET YOUR OWN EXPERIMENT SETUP ###\n",
    "\"\"\"\n",
    "Here, you may want to stop and think what is the best way to iterate(!) through all the models and experiments you want to try.\n",
    "Instead of running your code everytime you want to change something, you could try to list all your desired experiments and\n",
    "run them all sequentially in one go (gridsearch style).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have run our experiment(s), trying different pre-processing steps, models and parameters. To easily compare the results from our experiments, we can use the MLFlow interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been logging our experiments in our local server. We can access the UI by opening the localhost address in any browser. When opening the server in the browser, we should see something like this:\n",
    "\n",
    "![MLFlow UI](https://github.com/ginofazzi/testing/raw/0d3ffce58ee12f6d1c86b6a3c2ac628314e7b82e/MLFLowUI.png)\n",
    "\n",
    "In this example, we can see the list of all our experiments in the left, and for each experiment a table with all the different runs.\n",
    "\n",
    "**NOTE:**<em> This example contains only one run for the experiment. When logging multiple runs with different parameters/metrics, you will be able to easily compare them using the \"Chart View\". Make sure to try this out.</em>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tried many different models with different parameters, we might want to save the best one. To do this, we can use again the UI:\n",
    "\n",
    "1. In the Experiments list, select the model you want to save. In the Table view, select the version of that model (Run name).\n",
    "2. On the left, you will see all the experiment's run details (Parameters, metrics, and ML Project files). Click on \"Register Model\".\n",
    "3. Now, on the \"Models\" tab, you will be able to see your saved model.\n",
    "\n",
    "![MLFlow save model](https://github.com/ginofazzi/testing/raw/0d3ffce58ee12f6d1c86b6a3c2ac628314e7b82e/MLFLow-save_model-1.png)\n",
    "\n",
    "In case of doubt, you can check the documentation [HERE](https://mlflow.org/docs/latest/model-registry.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Using a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have saved our best model, we might want to use it for future predictions. We can retrieve the weather forecasts for the next days, and make predictions of power generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need the new forecast data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all future forecasts regardless of lead time\n",
    "forecasts  = client.query(\n",
    "    \"SELECT * FROM MetForecasts where time > now()\") # Query written in InfluxQL\n",
    "\n",
    "# Transform the result set into a dataframe\n",
    "forecasts = set_to_dataframe(forecasts)\n",
    "\n",
    "# Limit to only the newest source time, so we have the latest forecast only\n",
    "newest_forecasts = forecasts.loc[forecasts[\"Source_time\"] == forecasts[\"Source_time\"].max()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can retrieve any saved model and use it to predict on the new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model details\n",
    "model_name = \"LinearRegression-Example\"\n",
    "model_version = 1\n",
    "\n",
    "# Use MLFlow to load the saved model\n",
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# Use the saved model to predict the power generation\n",
    "predictions = model.predict(newest_forecasts[\"Speed\"])\n",
    "\n",
    "# We can transform the predictions array to a Pandas DataFrame\n",
    "predictions = pd.DataFrame(predictions, index=newest_forecasts.index, columns=[\"Power\"])\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain your model and keep the best one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we now how to retrieve saved models, it might be a good idea to validate our models troughout time, with new data, and keep the best one every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO -> INSERT YOUR CODE HERE ###\n",
    "\"\"\"\n",
    "You might want to think how to systematically test your model with new data, \n",
    "compare with the metrics of the best saved model, and update your model if \n",
    "necessary. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Deploying your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have trained several models and saved the best for predictions. But we have been working locally. Often times, we will want to deploy our model in such a way that other people can take advantage of it, by sending a request with data and getting back predictions. To do this, we need to create an endopoint for our model to receive data. This is done through deployment of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy our model, we first need to serve the model. Serving a model refers to the process of making a trained machine learning model available to receive input data and provide predictions or inferences based on that data. In other words, when you serve a model, you set it up in a way that it can be queried with new data, and it will produce predictions or outputs based on the patterns it learned during training.\n",
    "\n",
    "Serving a model is a critical step in the machine learning lifecycle, as it allows you to leverage the model's predictive capabilities in real-world applications. When a model is served, it becomes accessible to applications, websites, or other systems that need to utilize its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deploy your model, follow section 3.3 from the Assignment PDF. If you want to learn how to deploy models from the Jupyter Notebook, try out the tutorial notebook at examples/sklearn_elasticnet_wine/train.ipynb. You can find it on the [MLFlow Tutorial](https://mlflow.org/docs/latest/tutorials-and-examples/tutorial.html). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a1_lsda2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
